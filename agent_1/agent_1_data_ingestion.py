# -*- coding: utf-8 -*-
"""Agent1 data ingestion.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YUEB_bx3piFGJH7VTuGCpGuFBgjzWLWa
"""

#@title Load Python libraries
! pip install alpha_vantage -q

# pip install numpy
import numpy as np

# pip install torch
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset
from torch.utils.data import DataLoader

# pip install matplotlib
import matplotlib.pyplot as plt
from matplotlib.pyplot import figure

# pip install alpha_vantage
from alpha_vantage.timeseries import TimeSeries

print("All libraries loaded")

config = {
    "alpha_vantage": {
        "key": "API 63I9JJ2AOHGSPHW8",  # ‚Üê Here your key
        #"symbols":[ "AAPL", "GOOGL", "AMZN", "META", "NVDA","TSLA","MELI","VSTA","PBR","RIO","VALE","HMY","PFE","GSK","JPM"], # Also AAPL, MSFT, etc.
        "symbols":[ "AAPL", "GOOGL"],
        "outputsize": "full",  # 'full' if you want more data or 'compact' for 100 points
        "key_close": "4. close"
    },
    "data": {
        "window_size": 30,            # or the value you want
        "train_split_size": 0.8       # 80% training, 20% validation
    },
    "plots": {
        "show_plots": True,
        "xticks_interval": 30,
        "color_actual": "#001f3f",
        "color_train": "#3D9970",
        "color_val": "#0074D9",
        "color_pred_train": "#3D9970",
        "color_pred_val": "#0074D9",
        "color_pred_test": "#FF4136"
        },
      "model": {
        "input_size": 1, # since we are only using 1 feature, close price
        "num_lstm_layers": 2,
        "lstm_size": 32,
        "dropout": 0.2,
    },
    "training": {
        "device": "cuda" if torch.cuda.is_available() else "cpu",
        "batch_size": 64,
        "num_epoch": 100,
        "learning_rate": 0.01,
        "scheduler_step_size": 40,
    }

}

#@title Data preparation: acquiring financial market data from Alpha Vantage

def download_data(config, plot=False, start_date=None, end_date=None):
    ts = TimeSeries(key=config["alpha_vantage"]["key_close"])
    data, meta_data = ts.get_daily(
        symbol=config["alpha_vantage"]["symbols"],
        outputsize=config["alpha_vantage"]["outputsize"]
    )

    data_date = list(data.keys())
    data_date.sort()  # sort from oldest to newest

    # üéØ FILTER DATE RANGE if specified
    if start_date and end_date:
        data_date = [d for d in data_date if start_date <= d <= end_date]
    else:
    # ‚úÖ Solo tomar los √∫ltimos 5 d√≠as
        data_date = data_date[-5:]

    data_close_price = [float(data[date][config["alpha_vantage"]["key_close"]]) for date in data_date]
    data_close_price = np.array(data_close_price)

    num_data_points = len(data_date)
    display_date_range = f"from {data_date[0]} to {data_date[-1]}"
    print("Number data points:", num_data_points, display_date_range)

    if plot:
        fig = figure(figsize=(20, 5), dpi=80)
        fig.patch.set_facecolor((1.0, 1.0, 1.0))
        plt.plot(data_date, data_close_price, color=config["plots"]["color_actual"])
        xticks = [data_date[i] if (i % config["plots"]["xticks_interval"] == 0 or i == num_data_points - 1) else "" for i in range(num_data_points)]
        x = np.arange(0, len(xticks))
        plt.xticks(x, xticks, rotation='vertical')
        plt.title(f"Daily close price for {config['alpha_vantage']['symbols']}, {display_date_range}")
        plt.grid(axis='y', linestyle='--')
        plt.tight_layout()
        plt.show()

    return data_date, data_close_price, num_data_points, display_date_range

all_data = {}

for symbol in config["alpha_vantage"]["symbols"]:
    print(f"\nüîç Descargando datos para: {symbol}")
    original_symbols = config["alpha_vantage"]["symbols"]
    config["alpha_vantage"]["symbols"] = symbol

    # Assign the returned tuple directly to a variable
    downloaded_symbol_data = download_data(
        config,
        plot=config["plots"]["show_plots"]
    )

    config["alpha_vantage"]["symbols"] = original_symbols

    # Access elements from the returned tuple using integer indices
    all_data[symbol] = {
        "dates": downloaded_symbol_data[0],
        "prices": downloaded_symbol_data[1].tolist(),
        "num_points": downloaded_symbol_data[2],
        "range": downloaded_symbol_data[3]
    }

# This part will now work correctly:
print("\n‚úÖ Datos descargados:")
for k in all_data: # all_data is a dict as expected
    print(f"{k}: {all_data[k]['range']}, {all_data[k]['num_points']} puntos")

plt.figure(figsize=(20, 8))
plt.title("Precio de cierre diario por empresa")
plt.xlabel("Fecha")
plt.ylabel("Precio de cierre (USD)")

# Colores diferentes para cada acci√≥n (pueden repetirse si hay muchas)
# Use the updated way to access colormaps
colors = plt.colormaps['tab20'](np.linspace(0, 1, len(all_data)))

for idx, (symbol, data) in enumerate(all_data.items()):
    fechas = data["dates"]
    # Use the original prices for plotting, not the normalized ones
    precios = data["prices"]

    # Convertir fechas a objetos datetime para graficar mejor
    fechas_datetime = [np.datetime64(f) for f in fechas]

    # Assign a label to each plot for the legend
    plt.plot(fechas_datetime, precios, label=symbol, color=colors[idx])

plt.legend()
plt.grid(True, linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()

#@title Data preparation: normalizing raw financial data

class Normalizer():
    def __init__(self):
        self.mu = None
        self.sd = None

    def fit_transform(self, x):
        self.mu = np.mean(x, axis=(0), keepdims=True)
        self.sd = np.std(x, axis=(0), keepdims=True)
        normalized_x = (x - self.mu)/self.sd
        return normalized_x

    def inverse_transform(self, x):
        return (x*self.sd) + self.mu

# normalize
scaler = Normalizer()
normalized_data_close_price = scaler.fit_transform(data_close_price)

# Funci√≥n para calcular el RSI
def calculate_rsi(data, window=14):
    delta = data.diff(1)
    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()
    rs = gain / loss
    rsi = 100 - (100 / (1 + rs))
    return rsi

# Funci√≥n para calcular el EMA
def calculate_ema(data, span):
    return data.ewm(span=span, adjust=False).mean()

# This redefinition of download_data was incorrect and is removed.
# def download_data(config, plot=False):
#     return {
#         "dates": data_date,
#         "prices": data_close_price,
#         "num_points": num_data_points,
#         "range": display_date_range
#     }

# Import pandas if not already imported
import pandas as pd

# Descargar datos para todos los tickers a la vez
# This line is commented out and likely the source of the tuple assignment if it were active.
# all_data = download_data(config, plot=False) # This line is no longer needed as all_data is populated in the previous cell.

final_data = {}

# Iterate through the existing all_data dictionary populated in the previous cell
for ticker, data_dict in all_data.items():
    try:
        print(f"\nüîç Procesando datos para: {ticker}")

        data_date = data_dict["dates"]
        data_close_price = data_dict["prices"]

        # Normalize the data
        scaler = Normalizer()
        normalized_data_close_price = scaler.fit_transform(np.array(data_close_price).reshape(-1, 1))

        # Armar DataFrame con precios (using normalized prices)
        df = pd.DataFrame({
            'Fecha': pd.to_datetime(data_date),
            'Adj Close': normalized_data_close_price.flatten() # Use the flattened normalized data
        }).set_index('Fecha')

        # Calcular indicadores t√©cnicos
        # Ensure pandas functions like .diff(), .rolling(), .ewm() are available for the DataFrame
        df['RSI'] = calculate_rsi(df['Adj Close'], 14)
        df['EMA_10'] = calculate_ema(df['Adj Close'], 10)
        df['EMA_20'] = calculate_ema(df['Adj Close'], 20)
        df['EMA_40'] = calculate_ema(df['Adj Close'], 40)

        # Crear indicadores de compra y venta
        df['compra1'] = ((df['EMA_10'] > df['EMA_20']) &
                         (df['EMA_20'] > df['EMA_40']) &
                         (df['RSI'] < 60) &
                         (df['EMA_10'].shift(1) < df['EMA_20'].shift(1))).astype(int)

        df['compra2'] = ((df['EMA_10'] > df['EMA_20']) &
                         (df['EMA_20'] > df['EMA_40']) &
                         (df['RSI'] < 60) &
                         (df['EMA_20'].shift(1) < df['EMA_40'].shift(1))).astype(int)

        df['compra3'] = ((df['RSI'] > df['RSI'].shift(1)) &
                         (df['RSI'].shift(1) < 35)).astype(int)

        df['venta1'] = ((df['RSI'] > 70) & (df['Adj Close'] < df['EMA_10'])).astype(int)
        df['venta2'] = ((df['RSI'] > 70) & (df['Adj Close'] < df['EMA_20'])).astype(int)

        df = df.round(2)

        # Seleccionar √∫ltimos 3 d√≠as y agregar columna de fecha
        last_3_days = df[['RSI', 'EMA_10', 'EMA_20', 'EMA_40',
                          'compra1', 'compra2', 'compra3', 'venta1', 'venta2']].tail(3).copy()
        last_3_days['Fecha'] = last_3_days.index

        final_data[ticker] = last_3_days

    except Exception as e:
        print(f"‚ö†Ô∏è Error al procesar {ticker}: {e}")

#@title Adaptado para m√∫ltiples tickers

import gspread
from oauth2client.service_account import ServiceAccountCredentials
import pandas as pd
import os
from google.colab import drive
import numpy as np
import datetime
from alpha_vantage.timeseries import TimeSeries
import time

# Montar Google Drive
drive.mount('/content/drive')

# ========================
# CONFIG
# ========================
API_KEY = "TU_API_KEY_AQUI"  # <-- Reemplaz√° por tu API Key de Alpha Vantage
tickers = ['AAPL', 'MSFT', 'GOOGL']  # Pod√©s agregar m√°s s√≠mbolos
spreadsheet_id = "1tiQ4ghpX_FB7hyySSjT-pzjMM_Cua6knRoTAfkHTr_s"
json_file_path = '/content/drive/MyDrive/Colab Notebooks/Stocks/credentials.json'

# ========================
# FUNCIONES
# ========================

def calcular_indicadores(df):
    df['RSI'] = df['Close'].pct_change().rolling(14).mean()  # simplificado
    df['EMA_10'] = df['Close'].ewm(span=10).mean()
    df['EMA_20'] = df['Close'].ewm(span=20).mean()
    df['EMA_40'] = df['Close'].ewm(span=40).mean()
    df['compra1'] = (df['EMA_10'] > df['EMA_20']).astype(int)
    df['compra2'] = (df['EMA_10'] > df['EMA_40']).astype(int)
    df['compra3'] = ((df['EMA_10'] > df['EMA_20']) & (df['EMA_20'] > df['EMA_40'])).astype(int)
    df['venta1'] = (df['EMA_10'] < df['EMA_20']).astype(int)
    df['venta2'] = (df['EMA_10'] < df['EMA_40']).astype(int)
    return df

def guardar_en_sheets(df_to_save):
    scope = ["https://spreadsheets.google.com/feeds", "https://www.googleapis.com/auth/drive"]

    if not os.path.exists(json_file_path):
        print(f"‚ùå Archivo de credenciales no encontrado: {json_file_path}")
        return

    try:
        creds = ServiceAccountCredentials.from_json_keyfile_name(json_file_path, scope)
        client = gspread.authorize(creds)
        sheet = client.open_by_key(spreadsheet_id).get_worksheet(0)
        sheet.clear()

        df_to_save = df_to_save.replace({np.nan: None})
        df_to_save['Fecha'] = df_to_save['Fecha'].dt.strftime('%Y-%m-%d')

        sheet.update([df_to_save.columns.values.tolist()] + df_to_save.values.tolist())
        print("‚úÖ Datos guardados exitosamente en Google Sheets.")
    except Exception as e:
        print(f"‚ö†Ô∏è Error al guardar en Google Sheets: {e}")

# ========================
# PROCESAMIENTO POR TICKER
# ========================
ts = TimeSeries(key=API_KEY, output_format='pandas')
final_data = []

for ticker in tickers:
    try:
        # USAR get_daily (gratis)
        data, _ = ts.get_daily(symbol=ticker, outputsize='compact')
        data = data.reset_index().rename(columns={
            'date': 'Fecha',
            '4. close': 'Close'
        })
        data = data[['Fecha', 'Close']].copy()
        data['Fecha'] = pd.to_datetime(data['Fecha'])
        data = calcular_indicadores(data)
        data['Empresa'] = ticker
        final_data.append(data)
        print(f"‚úÖ Procesado: {ticker}")
        time.sleep(12)  # evitar l√≠mites de Alpha Vantage
    except Exception as e:
        print(f"‚ö†Ô∏è Error con {ticker}: {e}")

# ========================
# CONCATENAR Y GUARDAR
# ========================
if final_data:
    final_df = pd.concat(final_data)
    final_df = final_df[['Empresa', 'Fecha', 'RSI', 'EMA_10', 'EMA_20', 'EMA_40', 'compra1', 'compra2', 'compra3', 'venta1', 'venta2']]
    guardar_en_sheets(final_df)
else:
    print("‚ö†Ô∏è No se generaron datos.")
